{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b62b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding    \n",
    "import faiss\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from Bio import Entrez\n",
    "import time\n",
    "import xmltodict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80688f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path = r\"E:\\RAG_Models\\BioASQ-training13b\\training13b.json\"\n",
    "abstract_cache = Path(r\".\\Abstracts\")\n",
    "email = \"an80@illinois.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "719b872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_cache.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5f96293",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f29cc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_dataset_path,\"r\") as file:\n",
    "    bioasq_json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ca32171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_urls = []\n",
    "questions = []\n",
    "ground_truth = []\n",
    "for data in bioasq_json_data['questions'][:100]:\n",
    "   document_urls.extend(data.get('documents'))\n",
    "   question = data.get(\"body\")\n",
    "   questions.append(question)\n",
    "   ground_truth.append(data.get(\"ideal_answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "566928a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicate documents. Original number of documents: 1142\n",
      "Number of documents after filtering: 1136\n"
     ]
    }
   ],
   "source": [
    "print(f'Checking for duplicate documents. Original number of documents: {len(document_urls)}')\n",
    "document_urls = list(set(document_urls))\n",
    "print(f'Number of documents after filtering: {len(document_urls)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "df6a515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 100\n",
      "Number of ground truths: 100\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of questions: {len(questions)}')\n",
    "print(f'Number of ground truths: {len(ground_truth)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c5102e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioasq_df = pd.DataFrame(columns=['Question','Ground Truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "97bf28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioasq_df['Question'] = questions\n",
    "bioasq_df['Ground Truth'] = ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e5af7f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>[Coding sequence mutations in RET, GDNF, EDNRB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>List signaling molecules (ligands) that intera...</td>\n",
       "      <td>[The 7 known EGFR ligands  are: epidermal grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is the protein Papilin secreted?</td>\n",
       "      <td>[Yes,  papilin is a secreted protein]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are long non coding RNAs spliced?</td>\n",
       "      <td>[Long non coding RNAs appear to be spliced thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is RANKL secreted from the cells?</td>\n",
       "      <td>[Receptor activator of nuclear factor κB ligan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Is Hirschsprung disease a mendelian or a multi...   \n",
       "1  List signaling molecules (ligands) that intera...   \n",
       "2                   Is the protein Papilin secreted?   \n",
       "3                  Are long non coding RNAs spliced?   \n",
       "4                  Is RANKL secreted from the cells?   \n",
       "\n",
       "                                        Ground Truth  \n",
       "0  [Coding sequence mutations in RET, GDNF, EDNRB...  \n",
       "1  [The 7 known EGFR ligands  are: epidermal grow...  \n",
       "2              [Yes,  papilin is a secreted protein]  \n",
       "3  [Long non coding RNAs appear to be spliced thr...  \n",
       "4  [Receptor activator of nuclear factor κB ligan...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bioasq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ccacb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioasq_df.to_csv(\"bioasq_ground_truth.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0ef86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMID_RE = re.compile(r\"/pubmed/(\\d+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22200e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmid_from_url(url: str) -> str:\n",
    "    m = PMID_RE.search(url)\n",
    "    return m.group(1) if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3e926e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = [pmid_from_url(doc) for doc in document_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c50dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_fetch_pmids(pmids, batch = 200):\n",
    "    \"\"\"Return {pmid: {'title','abstract','year'}}; caches and skips empty abstracts.\"\"\"\n",
    "    out = {}\n",
    "    for i in range(0, len(pmids), batch):\n",
    "        chunk = pmids[i:i + batch]\n",
    "        need = [p for p in chunk if not (abstract_cache / f\"{p}.json\").exists()]\n",
    "        if need:\n",
    "            raw_xml = Entrez.efetch(\n",
    "                db=\"pubmed\",\n",
    "                id=\",\".join(need),\n",
    "                rettype=\"abstract\",\n",
    "                retmode=\"xml\"\n",
    "            ).read()\n",
    "            xml = xmltodict.parse(raw_xml)\n",
    "            for art in xml[\"PubmedArticleSet\"][\"PubmedArticle\"]:\n",
    "                pmid = art[\"MedlineCitation\"][\"PMID\"][\"#text\"]\n",
    "                art_info = art[\"MedlineCitation\"][\"Article\"]\n",
    "                title = art_info.get(\"ArticleTitle\", \"\")\n",
    "\n",
    "                # ---- robust abstract extraction ----\n",
    "                abs_raw = art_info.get(\"Abstract\", {}).get(\"AbstractText\", [])\n",
    "                if isinstance(abs_raw, list):\n",
    "                    parts = [x.get(\"#text\", \"\") if isinstance(x, dict) else str(x) for x in abs_raw]\n",
    "                    abstract = \" \".join(parts).strip()\n",
    "                else:\n",
    "                    abstract = str(abs_raw).strip()\n",
    "\n",
    "                if not abstract:   # skip if empty\n",
    "                    continue\n",
    "\n",
    "                year = art_info[\"Journal\"][\"JournalIssue\"][\"PubDate\"].get(\"Year\", \"Unknown\")\n",
    "\n",
    "                meta = {\"title\": title, \"abstract\": abstract, \"year\": year}\n",
    "                (abstract_cache / f\"{pmid}.json\").write_text(json.dumps(meta))\n",
    "\n",
    "        # load all cached (existing + newly saved)\n",
    "        for pmid in chunk:\n",
    "            fp = abstract_cache / f\"{pmid}.json\"\n",
    "            if fp.exists():\n",
    "                meta = json.loads(fp.read_text())\n",
    "                if meta.get(\"abstract\", \"\").strip():\n",
    "                    out[pmid] = meta\n",
    "        time.sleep(0.4)  # politeness\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4552b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = batch_fetch_pmids(pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b8b0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = r\".\\Abstracts\"\n",
    "out_csv = \"pubmed_abstracts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94cb3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_df = pd.DataFrame(columns=[\"title\", \"abstract\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c12bb32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(source_directory):\n",
    "    file_path = os.path.join(source_directory,file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        try:\n",
    "            json_data = json.loads(f.read())\n",
    "            title = json_data.get('title')   \n",
    "            abstract = json_data.get('abstract')\n",
    "            year = json_data.get('year')\n",
    "            abstracts_df.loc[len(abstracts_df)] = [title, abstract, year]     \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6dd4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_documents(df):\n",
    "    docs = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"abstract\"]\n",
    "        metadata = {\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"year\": row.get(\"year\", \"\"),\n",
    "        }\n",
    "        docs.append(Document(text=text, metadata=metadata))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e797bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_directory = os.path.join(os.getcwd(), out_csv)\n",
    "abstracts_df.to_csv(target_directory, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa4ca77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dataframe_to_documents(abstracts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2fa429fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = r\"E:\\RAG_Models\"\n",
    "model_name = \"phi-2-orange.Q4_K_M.gguf\"\n",
    "\n",
    "model_path = model_directory+\"\\\\\"+model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fd31467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 325 tensors from E:\\RAG_Models\\phi-2-orange.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
      "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50295\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 50256\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  195 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.66 GiB (5.14 BPW) \n",
      "load: missing pre-tokenizer type, using: 'default'\n",
      "load:                                             \n",
      "load: ************************************        \n",
      "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "load: CONSIDER REGENERATING THE MODEL             \n",
      "load: ************************************        \n",
      "load:                                             \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: special tokens cache size = 944\n",
      "load: token to piece cache size = 0.3151 MB\n",
      "print_info: arch             = phi2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 2560\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 32\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 80\n",
      "print_info: n_embd_head_v    = 80\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 2560\n",
      "print_info: n_embd_v_gqa     = 2560\n",
      "print_info: f_norm_eps       = 1.0e-05\n",
      "print_info: f_norm_rms_eps   = 0.0e+00\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 10240\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 2.78 B\n",
      "print_info: general.name     = Phi2\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 51200\n",
      "print_info: n_merges         = 50000\n",
      "print_info: BOS token        = 50256 '<|endoftext|>'\n",
      "print_info: EOS token        = 50295 '<|im_end|>'\n",
      "print_info: EOT token        = 50295 '<|im_end|>'\n",
      "print_info: UNK token        = 50256 '<|endoftext|>'\n",
      "print_info: PAD token        = 50256 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 50256 '<|endoftext|>'\n",
      "print_info: EOG token        = 50295 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 244 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =   787.50 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1704.63 MiB\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      ".............................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.20 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   640.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   167.01 MiB\n",
      "llama_context: graph nodes  = 1289\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.architecture': 'phi2', 'phi2.context_length': '2048', 'general.name': 'Phi2', 'tokenizer.ggml.padding_token_id': '50256', 'phi2.attention.head_count_kv': '32', 'phi2.embedding_length': '2560', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.feed_forward_length': '10240', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '50256', 'phi2.block_count': '32', 'phi2.attention.head_count': '32', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.rope.dimension_count': '32', 'tokenizer.ggml.eos_token_id': '50295', 'general.file_type': '15', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.unknown_token_id': '50256'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(model_path = model_path, temperature = 0.2, max_new_tokens = 256, context_window = 2048, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb569a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42e7b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25178a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "Settings.node_parser = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8edf43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = len(Settings.embed_model.get_text_embedding(\"sample text\"))\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(dimensions)\n",
    "faiss_db = FaissVectorStore(faiss_index = faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store = faiss_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0aa59c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2604634",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit = 600)\n",
    "query_engine = index.as_query_engine(similarity_top_k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7c6ec1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine = query_engine,\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b9754daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = bioasq_df['Question'][:3]\n",
    "sample_gt = bioasq_df['Ground Truth'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec48217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Hirschsprung disease a mendelian or a multifactorial disorder?\n",
      "List signaling molecules (ligands) that interact with the receptor EGFR?\n",
      "Is the protein Papilin secreted?\n"
     ]
    }
   ],
   "source": [
    "for query in sample_questions:\n",
    "    response = chat_engine.chat(query)\n",
    "    print(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9a0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
