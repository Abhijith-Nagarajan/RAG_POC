{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388bc2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhij\\miniconda3\\envs\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding    \n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf5079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./pubmed_abstracts/pubmed_abstracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77fb9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7402ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_documents(df):\n",
    "    docs = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"abstract\"]\n",
    "        metadata = {\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"year\": row.get(\"year\", \"\"),\n",
    "        }\n",
    "        docs.append(Document(text=text, metadata=metadata))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247ef7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dataframe_to_documents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53606c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = r\"E:\\RAG_Models\"\n",
    "model_name = \"phi-2-orange.Q4_K_M.gguf\"\n",
    "\n",
    "model_path = model_directory+\"\\\\\"+model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e13d3c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 325 tensors from E:\\RAG_Models\\phi-2-orange.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
      "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50295\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 50256\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  195 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.66 GiB (5.14 BPW) \n",
      "load: missing pre-tokenizer type, using: 'default'\n",
      "load:                                             \n",
      "load: ************************************        \n",
      "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "load: CONSIDER REGENERATING THE MODEL             \n",
      "load: ************************************        \n",
      "load:                                             \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: special tokens cache size = 944\n",
      "load: token to piece cache size = 0.3151 MB\n",
      "print_info: arch             = phi2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 2560\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 32\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 80\n",
      "print_info: n_embd_head_v    = 80\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 2560\n",
      "print_info: n_embd_v_gqa     = 2560\n",
      "print_info: f_norm_eps       = 1.0e-05\n",
      "print_info: f_norm_rms_eps   = 0.0e+00\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 10240\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 2.78 B\n",
      "print_info: general.name     = Phi2\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 51200\n",
      "print_info: n_merges         = 50000\n",
      "print_info: BOS token        = 50256 '<|endoftext|>'\n",
      "print_info: EOS token        = 50295 '<|im_end|>'\n",
      "print_info: EOT token        = 50295 '<|im_end|>'\n",
      "print_info: UNK token        = 50256 '<|endoftext|>'\n",
      "print_info: PAD token        = 50256 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 50256 '<|endoftext|>'\n",
      "print_info: EOG token        = 50295 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 244 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =   787.50 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1704.63 MiB\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      ".............................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.20 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   640.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   167.01 MiB\n",
      "llama_context: graph nodes  = 1289\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.architecture': 'phi2', 'phi2.context_length': '2048', 'general.name': 'Phi2', 'tokenizer.ggml.padding_token_id': '50256', 'phi2.attention.head_count_kv': '32', 'phi2.embedding_length': '2560', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.feed_forward_length': '10240', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '50256', 'phi2.block_count': '32', 'phi2.attention.head_count': '32', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.rope.dimension_count': '32', 'tokenizer.ggml.eos_token_id': '50295', 'general.file_type': '15', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.unknown_token_id': '50256'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(model_path = model_path, temperature = 0.2, max_new_tokens = 256, context_window = 2048, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1e122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d77ce08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhij\\miniconda3\\envs\\RAG\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhij\\AppData\\Local\\llama_index\\models--pritamdeka--BioBERT-mnli-snli-scinli-scitail-mednli-stsb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d143a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "Settings.node_parser = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8e605df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = len(Settings.embed_model.get_text_embedding(\"sample text\"))\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(dimensions)\n",
    "faiss_db = FaissVectorStore(faiss_index = faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store = faiss_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7eae7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b205b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit = 600)\n",
    "query_engine = index.as_query_engine(similarity_top_k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abaab3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine = query_engine,\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e614af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Is elevated dopamine consistently observed in patients with schizophrenia?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21fcfc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   31476.98 ms\n",
      "llama_perf_context_print: prompt eval time =   31389.17 ms /  1090 tokens (   28.80 ms per token,    34.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49319.13 ms /   255 runs   (  193.41 ms per token,     5.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   81638.62 ms /  1345 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the provided studies, there is evidence of elevated dopamine in patients with schizophrenia, particularly in midbrain regions that project to parts of the striatum. However, the relationship between dopamine and schizophrenia is complex, and the findings are not consistent across all studies. Some studies have found preliminary evidence of an association between chronic social stress exposure and increased striatal dopamine functioning, but the quality of available studies is generally low. Therefore, while there is some evidence of elevated dopamine in schizophrenia, the findings are not universally consistent.\n",
      "\n",
      "The studies also suggest that NM levels are higher in patients with schizophrenia than in healthy control individuals, which further supports the link between dopamine and schizophrenia. However, it's important to note that these findings are based on specific measures of dopamine and NM, and more research is needed to fully understand the relationship between dopamine and schizophrenia.\n",
      "\n",
      "In summary, while there is some evidence of elevated dopamine in patients with schizophrenia, the findings are not consistently observed across all studies, and the relationship between dopamine and schizophrenia is complex. Further research is needed to fully understand the role of dopamine in schizophrenia and its potential as a therapeutic target.\n",
      "\n",
      "It's also worth mentioning that the studies mentioned in the provided context information are not exhaustive, and there may be other studies that\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
